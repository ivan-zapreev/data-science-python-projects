{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Complaint Types\n",
    "\n",
    "The goal of this exercise is to do Model Development and Validation to find the answer to the Question 4 of the problem statement:\n",
    "\n",
    "> Can a predictive model be built for future prediction of the possibility of complaints of the specific type that you identified in response to Question 1?\n",
    "\n",
    "This exercise will be based on the findings of the previous three exercises. Therefore, we shall use the 311 complaints and the PLUTO data sets to feature-engineer a 'HEAT/HOT WATER' complaints for tax lots dataset. The latter is to be used to build a predictive model to estimate the number of future complaints based on selected house characteristics (which we will also refer to as properties or features).\n",
    "\n",
    "We shall formalize the question at hand as follows:\n",
    "\n",
    "> Build a prediction model for the number of 'HEAT/HOT WATER' complaints per year for a house with a selectd set of characteristics.\n",
    "\n",
    "The rest of the work will be organized as follows, we shall:\n",
    "1. Load, clean and prepare the data sets\n",
    "   * In a similar way we did for answering Questions 1 to 3\n",
    "2. Join the '311 complaint' data set with the PLUTO one\n",
    "   * In a similar way we did for answering Questions 1 to 3\n",
    "3. Determine the models to be used\n",
    "   * This will influence the feature selection process\n",
    "4. Perform house feature selection\n",
    "   * This will be re-done as we used a different model for Question 3\n",
    "5. Perform model training\n",
    "   * Including parameter tuning and cross validation if any\n",
    "6. Evaluate and compare models\n",
    "7. Recommending the best performing model\n",
    "\n",
    "Please note that, the data sets will not be described as the latter has already been done when answering Questions 1 to 3. We shall only repeat that the PLUTO data set will initially be taked with the following set of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial set of PLUTO features to consider:\n",
      " ['Address', 'BldgArea', 'BldgDepth', 'BuiltFAR', 'CommFAR', 'FacilFAR', 'Lot', 'LotArea', 'LotDepth', 'NumBldgs', 'NumFloors', 'OfficeArea', 'ResArea', 'ResidFAR', 'RetailArea', 'YearBuilt', 'YearAlter1', 'ZipCode', 'YCoord', 'XCoord']\n"
     ]
    }
   ],
   "source": [
    "pluto_features = ['Address', 'BldgArea', 'BldgDepth', 'BuiltFAR',\n",
    "              'CommFAR', 'FacilFAR', 'Lot', 'LotArea', 'LotDepth',\n",
    "              'NumBldgs', 'NumFloors', 'OfficeArea', 'ResArea',\n",
    "              'ResidFAR', 'RetailArea', 'YearBuilt', 'YearAlter1',\n",
    "              'ZipCode', 'YCoord', 'XCoord']\n",
    "print('The initial set of PLUTO features to consider:\\n', pluto_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load, clean, prepare\n",
    "\n",
    "Loading of the data can be done both from the IBM cloud storage and the locally present CSV files. The latter is decided upon the presence of the proper secure field values of the credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import seaborn\n",
    "import ibm_boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from botocore.client import Config\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "SECURITY_DUMMY = '----------------'\n",
    "erm2_nwe9_creds = {\n",
    "    'IAM_SERVICE_ID'    : SECURITY_DUMMY,\n",
    "    'IBM_API_KEY_ID'    : SECURITY_DUMMY,\n",
    "    'ENDPOINT'          : 'https://s3.eu-geo.objectstorage.service.networklayer.com',\n",
    "    'IBM_AUTH_ENDPOINT' : 'https://iam.eu-gb.bluemix.net/oidc/token',\n",
    "    'BUCKET'            : SECURITY_DUMMY,\n",
    "    'FILE'              : 'erm2_nwe9.csv'\n",
    "}\n",
    "bk_18v1_creds = {\n",
    "    'IAM_SERVICE_ID'    : SECURITY_DUMMY,\n",
    "    'IBM_API_KEY_ID'    : SECURITY_DUMMY,\n",
    "    'ENDPOINT'          : 'https://s3.eu-geo.objectstorage.service.networklayer.com',\n",
    "    'IBM_AUTH_ENDPOINT' : 'https://iam.eu-gb.bluemix.net/oidc/token',\n",
    "    'BUCKET'            : SECURITY_DUMMY,\n",
    "    'FILE'              : 'BK_18v1.csv'\n",
    "}\n",
    "bx_18v1_creds = {\n",
    "    'IAM_SERVICE_ID'    : SECURITY_DUMMY,\n",
    "    'IBM_API_KEY_ID'    : SECURITY_DUMMY,\n",
    "    'ENDPOINT'          : 'https://s3.eu-geo.objectstorage.service.networklayer.com',\n",
    "    'IBM_AUTH_ENDPOINT' : 'https://iam.eu-gb.bluemix.net/oidc/token',\n",
    "    'BUCKET'            : SECURITY_DUMMY,\n",
    "    'FILE'              : 'BX_18v1.csv'\n",
    "}\n",
    "mn_18v1_creds = {\n",
    "    'IAM_SERVICE_ID'    : SECURITY_DUMMY,\n",
    "    'IBM_API_KEY_ID'    : SECURITY_DUMMY,\n",
    "    'ENDPOINT'          : 'https://s3.eu-geo.objectstorage.service.networklayer.com',\n",
    "    'IBM_AUTH_ENDPOINT' : 'https://iam.eu-gb.bluemix.net/oidc/token',\n",
    "    'BUCKET'            : SECURITY_DUMMY,\n",
    "    'FILE'              : 'MN_18v1.csv'\n",
    "}\n",
    "qn_18v1_creds = {\n",
    "    'IAM_SERVICE_ID'    : SECURITY_DUMMY,\n",
    "    'IBM_API_KEY_ID'    : SECURITY_DUMMY,\n",
    "    'ENDPOINT'          : 'https://s3.eu-geo.objectstorage.service.networklayer.com',\n",
    "    'IBM_AUTH_ENDPOINT' : 'https://iam.eu-gb.bluemix.net/oidc/token',\n",
    "    'BUCKET'            : SECURITY_DUMMY,\n",
    "    'FILE'              : 'QN_18v1.csv'\n",
    "}\n",
    "si_18v1_creds = {\n",
    "    'IAM_SERVICE_ID'    : SECURITY_DUMMY,\n",
    "    'IBM_API_KEY_ID'    : SECURITY_DUMMY,\n",
    "    'ENDPOINT'          : 'https://s3.eu-geo.objectstorage.service.networklayer.com',\n",
    "    'IBM_AUTH_ENDPOINT' : 'https://iam.eu-gb.bluemix.net/oidc/token',\n",
    "    'BUCKET'            : SECURITY_DUMMY,\n",
    "    'FILE'              : 'SI_18v1.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows to get the data source for the credentials from the IBM cloud or local csv file \n",
    "def get_data_source(credentials) :\n",
    "    '''Creates a data source from the IBM cloud or local csv file according to the credentials'''\n",
    "    # Here we check if the credentials are present, if not try \n",
    "    # load the local file if they are then read from the cloud.\n",
    "    if credentials.get('IAM_SERVICE_ID') == SECURITY_DUMMY :\n",
    "        # This is the alternative to get the code run locally with a local csv file\n",
    "        body = 'data' + os.path.sep + credentials.get('FILE')\n",
    "    else :\n",
    "        client = ibm_boto3.client(\n",
    "            service_name = 's3',\n",
    "            ibm_api_key_id = credentials.get('IBM_API_KEY_ID'),\n",
    "            ibm_auth_endpoint = credentials.get('IBM_AUTH_ENDPOINT'),\n",
    "            config = Config(signature_version='oauth'),\n",
    "            endpoint_url = credentials.get('ENDPOINT'))\n",
    "\n",
    "        body = client.get_object(\n",
    "            Bucket = credentials.get('BUCKET'),\n",
    "            Key = credentials.get('FILE'))['Body']\n",
    "\n",
    "        # add missing __iter__ method, so pandas accepts body as file-like object\n",
    "        def __iter__(self): return 0\n",
    "        if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n",
    "\n",
    "    return body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, we shall subsequently load the 311 and PLUTO data sets. Along the way, we will select the necessary columns and check on (, and correct if needed,) the column data types.\n",
    "\n",
    "## 311 complaints\n",
    "\n",
    "Balow we load the data set, and then first select the required complaints along with the needed columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all complaints: 6034470\n"
     ]
    }
   ],
   "source": [
    "# Get the data source for the credentials\n",
    "dhp_ds = get_data_source(erm2_nwe9_creds)\n",
    "\n",
    "# Read the CSV file\n",
    "dhp_df = pd.read_csv(dhp_ds, parse_dates = ['created_date', 'closed_date'])\n",
    "print('Number of all complaints:', dhp_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'HEAT/HOT WATER' complaints is: 2159103\n"
     ]
    }
   ],
   "source": [
    "# Select the 'HEAT/HOT WATER' complaints\n",
    "dhp_df = dhp_df[(dhp_df['complaint_type'] == 'HEAT/HOT WATER')]\n",
    "print('Number of \\'HEAT/HOT WATER\\' complaints is:', dhp_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns that matter, and rename for convenience\n",
    "dhp_df = dhp_df[['created_date', 'incident_address', 'incident_zip']]\n",
    "dhp_df = dhp_df.rename({'incident_address':'Address', 'incident_zip':'ZipCode'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the address to upper case for uniformity\n",
    "dhp_df.Address = dhp_df.Address.map(str).map(str.upper)\n",
    "# Strip the address strings\n",
    "dhp_df.Address = dhp_df.Address.str.strip()\n",
    "# Replace sequence of white spaces with one\n",
    "dhp_df.Address = dhp_df.Address.str.replace('\\s+',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before dropping Na/NaN: 2159103 , after: 2140078\n"
     ]
    }
   ],
   "source": [
    "# Drop the Na/NaN valued rows\n",
    "init_size = dhp_df.shape[0]\n",
    "dhp_df.dropna(inplace = True)\n",
    "print('Number of rows before dropping Na/NaN:', init_size,', after:', dhp_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exract the year the complaint was created and then drop the *'created_date'* column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhp_df['Year'] = dhp_df.created_date.dt.year\n",
    "dhp_df.drop(columns = ['created_date'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us summarize the Year statitics so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2140078\n",
       "mean        2014\n",
       "std            2\n",
       "min         2010\n",
       "25%         2012\n",
       "50%         2015\n",
       "75%         2017\n",
       "max         2020\n",
       "Name: Year, dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_descr = dhp_df['Year'].describe().astype(int)\n",
    "year_descr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see the min/max years range is between `2010` and `2020` which means that there were no missing/wrong *'created_date'* values present.\n",
    "\n",
    "We will only need the average complaint counts per year for each given *'Address'*/*'ZipCode'* pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ZipCode</th>\n",
       "      <th>Address</th>\n",
       "      <th>AvgCnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001.0</td>\n",
       "      <td>10 WEST 28 STREET</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001.0</td>\n",
       "      <td>100 WEST 26 STREET</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001.0</td>\n",
       "      <td>102 WEST 29 STREET</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10001.0</td>\n",
       "      <td>103 WEST 27 STREET</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10001.0</td>\n",
       "      <td>11 WEST 34 STREET</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ZipCode             Address    AvgCnt\n",
       "0  10001.0   10 WEST 28 STREET  0.090909\n",
       "1  10001.0  100 WEST 26 STREET  0.090909\n",
       "2  10001.0  102 WEST 29 STREET  0.090909\n",
       "3  10001.0  103 WEST 27 STREET  0.090909\n",
       "4  10001.0   11 WEST 34 STREET  0.090909"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by the zip code and address to count the complaints\n",
    "dhp_df = dhp_df.groupby(['ZipCode', 'Address']).size().to_frame()\n",
    "\n",
    "# Rename the counts column and then compute the average count for the min/max years range\n",
    "dhp_df.rename({0 : 'AvgCnt'}, axis = 1, inplace = True)\n",
    "dhp_df.AvgCnt = dhp_df.AvgCnt/(year_descr.loc['max'] - year_descr.loc['min'] + 1)\n",
    "\n",
    "# Re-set the indexes to turn the Address and ZipCode back into columns\n",
    "dhp_df.reset_index(level = 1, inplace = True)\n",
    "dhp_df.reset_index(level = 0, inplace = True)\n",
    "dhp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we check on the column types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ZipCode    float64\n",
       "Address     object\n",
       "AvgCnt     float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dhp_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the data types are in order: the address is a string and the zip code and average complaint count are floats.\n",
    "\n",
    "## PLUTO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Join data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Univariate Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Feature correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Final selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Evaluation summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
